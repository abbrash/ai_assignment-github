{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence and Expert Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment II: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Explain the basic architecture of an RNN and how it is different from a feedforward neural network. What are the advantages and disadvantages of using an RNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic architecture of a Recurrent Neural Network (RNN) consists of recurrent connections that allow information to be passed from previous steps to the current step, making it suitable for processing sequential data. The key difference between an RNN and a feedforward neural network (FNN) lies in their treatment of sequential data.\n",
    "\n",
    "In a typical RNN, the input at each time step is not only influenced by the current input but also by the hidden state from the previous time step. The hidden state acts as a memory that captures the information from past steps, enabling the network to maintain context and make predictions based on the entire sequence.\n",
    "\n",
    "The key advantages of using an RNN are:\n",
    "\n",
    "1. Sequential Processing: \n",
    "\n",
    "    RNNs are designed to process sequential data such as time series, speech, and natural language. They can capture temporal dependencies and handle inputs of varying lengths, making them suitable for tasks where the order of the input matters.\n",
    "\n",
    "2. Variable-Length Input: \n",
    "\n",
    "    RNNs can handle inputs of different lengths within the same model architecture. This flexibility is particularly useful for tasks where the length of the sequence may vary, such as text classification or speech recognition.\n",
    "\n",
    "3. Memory and Context: \n",
    "\n",
    "    The recurrent connections in an RNN allow the network to maintain memory and context over time. This capability is beneficial for tasks requiring a historical context, such as language modeling or sentiment analysis.\n",
    "\n",
    "Despite their advantages, RNNs also have some limitations and disadvantages:\n",
    "\n",
    "1. Vanishing/Exploding Gradient: \n",
    "\n",
    "    RNNs can suffer from the vanishing or exploding gradient problem, where the gradient signal diminishes or amplifies exponentially as it propagates back through time. This issue can make training RNNs challenging and lead to difficulties in capturing long-term dependencies.\n",
    "\n",
    "2. Computational Complexity: \n",
    "\n",
    "    RNNs can be computationally expensive, especially when dealing with long sequences. The sequential nature of RNNs limits parallelization, which can result in slower training and inference times compared to feedforward networks.\n",
    "\n",
    "3. Short-Term Memory: \n",
    "\n",
    "    Traditional RNNs have limitations in capturing long-term dependencies due to the vanishing gradient problem. Although solutions like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been introduced to mitigate this issue, they can be more complex to train and require more parameters.\n",
    "\n",
    "4. Limited Context: \n",
    "\n",
    "    RNNs primarily focus on the context from previous time steps and may struggle with capturing dependencies that are distant in the sequence. Tasks that require a broader context, such as document understanding, might benefit from other architectures like Transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. What is the problem of vanishing and exploding gradients in RNNs? How can this problem be mitigated?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem is essentially a situation in which a deep multilayer feed-forward network or a recurrent neural network (RNN) does not have the ability to propagate useful gradient information from the output end of the model back to the layers near the input end of the model.\n",
    "\n",
    "It results in models with many layers being rendered unable to learn on a specific dataset. It could even cause models with many layers to prematurely converge to a substandard solution.\n",
    "\n",
    "When the backpropagation algorithm advances downwards(or backward) going from the output layer to the input layer, the gradients tend to shrink, becoming smaller and smaller till they approach zero. This ends up leaving the weights of the initial or lower layers practically unchanged. In this situation, the gradient descent does not ever end up converging to the optimum. \n",
    "\n",
    "Vanishing gradient does not necessarily imply that the gradient vector is all zero (with the exception of numerical overflow). It implies that the gradients are minuscule, which would cause the learning to be very slow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods that are proposed to overcome the vanishing gradient problem:\n",
    "\n",
    "* Residual neural networks (ResNets)\n",
    "* Multi-level hierarchy\n",
    "* Long short term memory (LSTM)\n",
    "* Faster hardware\n",
    "* ReLU\n",
    "* Batch normalization\n",
    "\n",
    "\n",
    "##### 1. Residual neural networks (ResNets)\n",
    "This is one of the most effective techniques that you can use to overcome the vanishing gradient problem. Before ResNets, a deeper network would have a higher degree of training error than a shallow network.\n",
    "\n",
    "A backpropagation algorithm each weight of the neural network in a manner that causes it to take a step in the direction along which the loss decreases. This direction is the gradient of the weight (concerning the loss).\n",
    "\n",
    "You can use the chain rule to find this gradient for each weight. You can find it by multiplying the local gradient by the gradient flowing from ahead.\n",
    "\n",
    "As the gradient flows backward to the  initial layers, this value goes on getting multiplied by each local gradient. This causes the gradient to keep shrinking, causing the updates to the initial layers to be rather minor, thus increasing the training time substantially. This problem could be solved if the local gradient managed to become 1.\n",
    "\n",
    "This can be achieved by using the identity function as its derivative would always be 1. So, the gradient would not decrease in value because the local gradient is 1.\n",
    "\n",
    "TheResNet architecture does not allow the vanishing gradient problem to occur. It has skip connections that function as gradient superhighways which allow the gradient to flow unhindered. It makes it possible for gradients to propagate to deep layers before they can are attenuated to small or zero values.\n",
    "\n",
    "##### 2. ReLU\n",
    "Here, we replace the typical sigmoidal activation functions used for node output with with a new function: f(x) = max(0, x). This activation only saturates in one direction and thus it is more resilient to the problem of vanishing of gradients.\n",
    "\n",
    "ReLU is widely used because it maps x to max(0,x) and does not squash the inputs to a small range though it squashes all the negative inputs to zero.\n",
    "\n",
    "However, ReLU is not the best option for the intermediate layers of the network in certain cases. There is the problem of dying ReLUs in which some neurons just end up dying out. This means that they end up throwing 0 as outputs with the advancement in training.\n",
    "\n",
    "Some alternative functions of the ReLU that overcome the vanishing gradient problem when used as activation for the intermediate layers of the network include LReLU, PReLU, ELU, SELU.\n",
    "\n",
    "##### 3. Multi-level hierarchy \n",
    "Multi-level hierarchy involves pre-training a single layer at a time and then performing backpropagation for fine-tuning.\n",
    "\n",
    "##### 4. Long Short Term Memory\n",
    "Long Short Term Memory (LSTM) was created specifically for the purpose of preventing the vanishing gradient problem. It manages to do that with the Constant Error Carousel (CEC). However, even in an LSTM, the gradients do tend to vanish; they just vanish at a far slower pace than they do in regular recurring neural networks.\n",
    "\n",
    "##### 5. Batch normalization\n",
    "The internal covariate shift problem is somewhat involved in the exploding gradient problem. Batch normalization can solve this problem. It involves normalizing the activations of each layer, making it possible for every layer to learn on a more stable distribution of inputs, thereby accelerating the training of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Explain the difference between simple RNNs, LSTM, and GRU. In what situations might each type of RNN be most appropriate?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Simple RNNs: \n",
    "Simple RNNs are the basic form of RNNs, where the hidden state at each time step is calculated based on the current input and the previous hidden state. However, simple RNNs suffer from the vanishing gradient problem, limiting their ability to capture long-term dependencies. Simple RNNs are most appropriate for tasks where short-term dependencies or local patterns are important, such as language modeling for short sentences or sequence classification with limited context.\n",
    "\n",
    "##### 2. LSTM (Long Short-Term Memory): \n",
    "LSTM networks were introduced to address the vanishing gradient problem in RNNs by incorporating memory cells and gating mechanisms. LSTM has an internal memory state that allows the network to selectively retain or forget information over time. It consists of three gates: the input gate, forget gate, and output gate. These gates control the flow of information and gradients, enabling LSTMs to capture long-term dependencies effectively. LSTMs are particularly useful for tasks that require modeling complex sequences with long-term dependencies, such as language translation, sentiment analysis, or speech recognition.\n",
    "\n",
    "##### 3. GRU (Gated Recurrent Unit): \n",
    "GRU is another variant of RNNs that also addresses the vanishing gradient problem. GRU simplifies the LSTM architecture by combining the memory cell and hidden state into a single \"update gate.\" It also introduces a \"reset gate\" to control the information flow and gradients. GRU has fewer parameters than LSTM, making it computationally more efficient. GRUs perform similarly to LSTMs in capturing long-term dependencies and are especially useful when computational resources are a concern or when dealing with moderately complex sequential tasks.\n",
    "\n",
    "In summary, the choice between simple RNNs, LSTM, and GRU depends on the specific characteristics of the problem and data:\n",
    "\n",
    "* Use simple RNNs when the task involves short-term dependencies and local patterns, and when computational resources are limited.\n",
    "\n",
    "* Use LSTM when dealing with complex sequences that require capturing long-term dependencies, especially in tasks such as language translation, sentiment analysis, or speech recognition.\n",
    "\n",
    "* Use GRU when computational efficiency is a concern, or when the task involves moderately complex sequential dependencies and long-term context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Explain how RNNs are trained using backpropagation through time (BPTT). What are some of the challenges associated with training RNNs, and how can these challenges be addressed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are trained using a variant of backpropagation called Backpropagation Through Time (BPTT). BPTT extends the traditional backpropagation algorithm to handle the temporal nature of sequential data and recurrent connections in RNNs. Here's an overview of how BPTT works:\n",
    "\n",
    "##### 1. Forward Pass: \n",
    "The input sequence is fed to the RNN one time step at a time. The hidden state at each time step is computed based on the current input and the previous hidden state. The output at each time step can be obtained from the hidden state using an output layer or by directly using the hidden state.\n",
    "\n",
    "##### 2. Loss Calculation: \n",
    "The output at each time step is compared to the target output to compute the loss. The losses at each time step are usually summed or averaged to obtain a single loss value for the entire sequence.\n",
    "\n",
    "##### 3. Backward Pass: \n",
    "The gradients are computed by propagating the loss backward through time. Starting from the final time step, the gradients are calculated with respect to the parameters of the model, including the weights connecting the input to the hidden state and the weights connecting the hidden state to itself. The gradients are accumulated over all time steps.\n",
    "\n",
    "##### 4. Gradient Update: \n",
    "Once the gradients have been computed, the parameters of the RNN are updated using an optimization algorithm such as stochastic gradient descent (SGD) or Adam. The gradients are used to determine the direction and magnitude of the updates.\n",
    "\n",
    "Challenges associated with training RNNs using BPTT:\n",
    "\n",
    "##### 1. Vanishing and Exploding Gradients: \n",
    "RNNs are prone to the vanishing and exploding gradient problems, where the gradients diminish or explode as they propagate through time. This can result in unstable training or difficulties in capturing long-term dependencies. Techniques like gradient clipping, careful weight initialization, and the use of specialized architectures like LSTM and GRU can help alleviate these issues.\n",
    "\n",
    "##### 2. Training Time and Computational Complexity: \n",
    "Training RNNs can be computationally expensive and time-consuming, especially for long sequences or deep architectures. Parallelization techniques, such as mini-batch processing and GPU utilization, can speed up training. Additionally, techniques like truncated BPTT, where the sequence is split into shorter subsequences, can reduce the computational burden.\n",
    "\n",
    "##### 3. Overfitting: \n",
    "RNNs are prone to overfitting, especially when the model capacity is high and the training data is limited. Regularization techniques like dropout and weight decay can help mitigate overfitting in RNNs.\n",
    "\n",
    "##### 4. Choosing Optimal Sequence Length: \n",
    "The length of the sequence to use during training is an important consideration. Using very long sequences can lead to memory and computational limitations, while very short sequences may not capture the desired dependencies. Finding an appropriate sequence length or utilizing techniques like bucketing or padding can address this challenge.\n",
    "\n",
    "##### 5. Exploration of Hyperparameters: \n",
    "RNN training involves tuning various hyperparameters, such as learning rate, batch size, and regularization strength. Efficient hyperparameter search techniques like grid search, random search, or Bayesian optimization can help find optimal configurations.\n",
    "\n",
    "Addressing these challenges often requires a combination of careful architecture design, appropriate regularization techniques, efficient optimization algorithms, and hyperparameter tuning. It is essential to experiment with different approaches and iterate to find the best solutions for training RNNs effectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. What happens to the gradient if you backpropagate through a long sequence?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When backpropagating through a long sequence in a Recurrent Neural Network (RNN), the gradients can suffer from the vanishing or exploding gradient problem. The behavior of the gradient depends on the specific RNN architecture, activation functions, and training settings. Here's an overview of what can happen:\n",
    "\n",
    "##### 1. Vanishing Gradient: \n",
    "In some cases, the gradients may gradually diminish as they propagate through time. This is known as the vanishing gradient problem. When the gradients become very small, the updates to the network's parameters become negligible, and the network fails to learn or capture long-term dependencies effectively. The vanishing gradient problem is more pronounced in traditional RNNs with simple activation functions like the sigmoid function.\n",
    "\n",
    "##### 2. Exploding Gradient: \n",
    "In other cases, the gradients can grow exponentially as they propagate through time. This is referred to as the exploding gradient problem. When the gradients become extremely large, weight updates can become unstable, and the network parameters may diverge. The exploding gradient problem is especially common when the network has unstable dynamics or when the weights are initialized poorly.\n",
    "\n",
    "The vanishing and exploding gradient problems hinder the training process and make it difficult for RNNs to capture long-term dependencies accurately. As a result, it becomes challenging to propagate information effectively over long sequences.\n",
    "\n",
    "To mitigate these issues, several techniques have been developed:\n",
    "\n",
    "##### 1. Gradient Clipping: \n",
    "Gradient clipping involves rescaling the gradients if they exceed a predefined threshold. By limiting the magnitude of the gradients, gradient clipping helps prevent the exploding gradient problem and stabilizes the training process.\n",
    "\n",
    "##### 2. Initialization Strategies: \n",
    "Proper initialization of the network's weights can alleviate the vanishing and exploding gradient problems. Techniques like Xavier initialization or He initialization help set the initial weights to appropriate scales, improving the stability of gradient propagation.\n",
    "\n",
    "##### 3. Gated Architectures: \n",
    "Specialized RNN architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have gating mechanisms that allow the network to control the flow of information over time. These architectures are designed to alleviate the vanishing gradient problem and can better capture long-term dependencies.\n",
    "\n",
    "##### 4. Truncated Backpropagation Through Time: \n",
    "In practice, training an RNN on very long sequences can be computationally expensive and suffer from memory limitations. Truncated Backpropagation Through Time involves breaking the long sequence into shorter subsequences and performing backpropagation on each subsequence separately. This helps address the computational challenges associated with training on long sequences.\n",
    "\n",
    "By employing these techniques, the vanishing and exploding gradient problems can be mitigated, improving the training stability and the RNN's ability to capture long-term dependencies effectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Train a network using simple RNN cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split \n",
    "from matplotlib import pyplot as plt\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Bidirectional\n",
    "\n",
    "dataset = pd.read_csv('Human_activity_Dataset.csv')     # shape = (10299, 563)\n",
    "\n",
    "dataset = dataset.drop(columns=['subject/Participant'])\n",
    "\n",
    "dataset = pd.get_dummies(dataset, columns=['Activity'])\n",
    "dataset_train, dataset_valid_test = train_test_split(dataset, test_size=0.4)\n",
    "dataset_valid, dataset_test = train_test_split(dataset_valid_test, test_size=0.5)\n",
    "\n",
    "labels_train = dataset_train[['Activity_LAYING', 'Activity_SITTING', 'Activity_STANDING', \n",
    "                              'Activity_WALKING', 'Activity_WALKING_DOWNSTAIRS', 'Activity_WALKING_UPSTAIRS']]\n",
    "\n",
    "labels_valid = dataset_valid[['Activity_LAYING', 'Activity_SITTING', 'Activity_STANDING', \n",
    "                              'Activity_WALKING', 'Activity_WALKING_DOWNSTAIRS', 'Activity_WALKING_UPSTAIRS']]\n",
    "\n",
    "labels_test = dataset_test[['Activity_LAYING', 'Activity_SITTING', 'Activity_STANDING', \n",
    "                            'Activity_WALKING', 'Activity_WALKING_DOWNSTAIRS', 'Activity_WALKING_UPSTAIRS']]\n",
    "\n",
    "features_train = dataset_train.drop(columns=labels_train)\n",
    "features_valid = dataset_valid.drop(columns=labels_valid)\n",
    "features_test = dataset_test.drop(columns=labels_test)\n",
    "\n",
    "input_neuron_num = (features_train).shape[1]           # 561:   excluded columns: 'Activity' and 'subject/Participant'    \n",
    "output_neuron_num = 6                                  # 6:     There are six neurons in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_3d = np.expand_dims(features_train, axis=1)\n",
    "features_valid_3d = np.expand_dims(features_valid, axis=1)\n",
    "features_test_3d = np.expand_dims(features_test, axis=1)\n",
    "\n",
    "epoch_size = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Define the SimpleRNN model\n",
    "model_simple_rnn = Sequential(name='Simple_RNN')\n",
    "model_simple_rnn.add(SimpleRNN(128, \n",
    "                               input_shape=(None, input_neuron_num), name='Hidden_Layer'))\n",
    "model_simple_rnn.add(Dense(output_neuron_num, \n",
    "                           activation='softmax', name='Output_Layer'))\n",
    "\n",
    "# Compile the SimpleRNN model\n",
    "model_simple_rnn.compile(loss='categorical_crossentropy', \n",
    "                         optimizer='adam', \n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the SimpleRNN model\n",
    "result_hist_simple_rnn = model_simple_rnn.fit(features_train_3d, \n",
    "                                              labels_train, \n",
    "                                              epochs=epoch_size, \n",
    "                                              batch_size=batch_size, \n",
    "                                              validation_data=(features_valid_3d, labels_valid))\n",
    "\n",
    "pd.DataFrame(result_hist_simple_rnn.history).plot(figsize=(10, 5), \n",
    "                                                  grid=True, \n",
    "                                                  xlabel=\"Epoch\", \n",
    "                                                  ylabel=\"Loss / Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "simple_rnn_eval = model_simple_rnn.evaluate(features_test_3d, \n",
    "                                            labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Train a network using LSTM Cells or GRU (you can use both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bidirectional GRU model\n",
    "model_bi_gru = Sequential()\n",
    "model_bi_gru.add(GRU(128, \n",
    "                 input_shape=(None, input_neuron_num)))\n",
    "model_bi_gru.add(Dense(output_neuron_num, \n",
    "                       activation='softmax'))\n",
    "\n",
    "# Compile the Bidirectional GRU model\n",
    "model_bi_gru.compile(loss='categorical_crossentropy', \n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the Bidirectional GRU model\n",
    "result_hist_bi_gru = model_bi_gru.fit(features_train_3d, \n",
    "                                      labels_train, \n",
    "                                      epochs=epoch_size, \n",
    "                                      batch_size=batch_size, \n",
    "                                      validation_data=(features_valid_3d, labels_valid))\n",
    "\n",
    "pd.DataFrame(result_hist_bi_gru.history).plot(figsize=(10, 5), \n",
    "                                              grid=True, \n",
    "                                              xlabel=\"Epoch\", \n",
    "                                              ylabel=\"Loss / Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "bi_gru_eval = model_bi_gru.evaluate(features_test_3d, \n",
    "                                    labels_test)\n",
    "\n",
    "# Define the Bidirectional LSTM model\n",
    "model_bi_lstm = Sequential()\n",
    "model_bi_lstm.add(LSTM(128, \n",
    "                  input_shape=(None, input_neuron_num)))\n",
    "model_bi_lstm.add(Dense(output_neuron_num, \n",
    "                        activation='softmax'))\n",
    "\n",
    "# Compile the Bidirectional LSTM model\n",
    "model_bi_lstm.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the Bidirectional LSTM model\n",
    "result_hist_bi_lstm = model_bi_lstm.fit(features_train_3d, \n",
    "                                        labels_train, \n",
    "                                        epochs=epoch_size, \n",
    "                                        batch_size=batch_size, \n",
    "                                        validation_data=(features_valid_3d, labels_valid))\n",
    "\n",
    "pd.DataFrame(result_hist_bi_lstm.history).plot(figsize=(10, 5), \n",
    "                                               grid=True, \n",
    "                                               xlabel=\"Epoch\", \n",
    "                                               ylabel=\"Loss / Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "bi_lstm_eval = model_bi_lstm.evaluate(features_test_3d, \n",
    "                                      labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII. Train a network using bidirectional LSTM or GRU ( you can use both). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Bidirectional(GRU(128), \n",
    "                            input_shape=(None, input_neuron_num)))\n",
    "model_gru.add(Dense(output_neuron_num, \n",
    "                    activation='softmax'))\n",
    "\n",
    "# Compile the GRU model\n",
    "model_gru.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the GRU model\n",
    "result_hist_gru = model_gru.fit(features_train_3d, \n",
    "                                labels_train, \n",
    "                                epochs=epoch_size, \n",
    "                                batch_size=batch_size, \n",
    "                                validation_data=(features_valid_3d, labels_valid))\n",
    "\n",
    "pd.DataFrame(result_hist_gru.history).plot(figsize=(10, 5), \n",
    "                                           grid=True, \n",
    "                                           xlabel=\"Epoch\", \n",
    "                                           ylabel=\"Loss / Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "gru_eval = model_gru.evaluate(features_test_3d, \n",
    "                              labels_test)\n",
    "\n",
    "# Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Bidirectional(LSTM(128), \n",
    "                             input_shape=(None, input_neuron_num)))\n",
    "model_lstm.add(Dense(output_neuron_num, \n",
    "                     activation='softmax'))\n",
    "\n",
    "# Compile the LSTM model\n",
    "model_lstm.compile(loss='categorical_crossentropy', \n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "result_hist_lstm = model_lstm.fit(features_train_3d, \n",
    "                                  labels_train, \n",
    "                                  epochs=epoch_size, \n",
    "                                  batch_size=batch_size, \n",
    "                                  validation_data=(features_valid_3d, labels_valid))\n",
    "\n",
    "pd.DataFrame(result_hist_lstm.history).plot(figsize=(10, 5), \n",
    "                                            grid=True, \n",
    "                                            xlabel=\"Epoch\", \n",
    "                                            ylabel=\"Loss / Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "lstm_eval = model_lstm.evaluate(features_test_3d, \n",
    "                                labels_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX. Tabular the results, compare them and write your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "           'Model': ['SimpleRNN', 'GRU', 'LSTM', 'Bidirectional GRU', 'Bidirectional LSTM'],\n",
    "           'Loss': [simple_rnn_eval[0], gru_eval[0], lstm_eval[0], bi_gru_eval[0], bi_lstm_eval[0]],\n",
    "           'Accuracy': [simple_rnn_eval[1], gru_eval[1], lstm_eval[1], bi_gru_eval[1], bi_lstm_eval[1]]\n",
    "          }\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the table\n",
    "print(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X. Bonus: There's an intriguing paper that presents the Forward-Forward algorithm. This algorithm replaces the forward and backward passes of backpropagation with two forward passes. The first pass involves positive (i.e., real) data, while the second pass involves negative data that can be generated by the network itself. Explore the intriguing aspects of this algorithm by delving into the paper. (bearing in mind that it was published in 2022, after Chatgpt knowledge cutoff date of 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn.summary()\n",
    "model_gru.summary()\n",
    "model_lstm.summary()\n",
    "model_bi_gru.summary()\n",
    "model_bi_lstm.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
